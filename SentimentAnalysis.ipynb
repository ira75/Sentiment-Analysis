{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyXsLFytVLt1"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE9lOI3FvCGR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "22fef5fe-ef5d-41c5-82c7-a73bbca2a691"
      },
      "source": [
        "from keras.utils import get_file\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import datasets, layers, models,losses\n",
        "from keras.optimizers import Adam\n",
        "import tarfile\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIONdGVgVQLL"
      },
      "source": [
        "Download zip data file and extract data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6p6XC1b_47f"
      },
      "source": [
        "data_dir = get_file('aclImdb_v1.tar.gz', 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', cache_subdir = \"datasets\",hash_algorithm = \"auto\", extract = True, archive_format = \"auto\")\n",
        "\n",
        "my_tar = tarfile.open(data_dir)\n",
        "my_tar.extractall('./data/') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otaKQfYBVhdV"
      },
      "source": [
        "Read train data while preprocessing and removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ-8_FlwqZS7"
      },
      "source": [
        "data_folder=\"/train\"\n",
        "imdb_folder = \"/aclImdb\"\n",
        "train_reviews = []\n",
        "train_labels = []\n",
        "ps = PorterStemmer()\n",
        "for index,sentiment in enumerate([\"/neg/\", \"/pos/\"]):\n",
        "    path = \"./data\"+imdb_folder + data_folder + sentiment\n",
        "    for filename in sorted(os.listdir(path)):\n",
        "        i=i+1\n",
        "        with open(path + filename, 'r') as f:\n",
        "            review = f.read()\n",
        "            review = review.lower()\n",
        "            review = review.rstrip()\n",
        "            review = review.replace(\"<br />\", \" \")\n",
        "            review = re.sub(r\"[^a-z ]\", \" \", review) \n",
        "            review = re.sub(r\" +\", \" \", review)\n",
        "            word_tokens = word_tokenize(review) \n",
        "            #remove stop words\n",
        "            filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "            filtered_sentence = [] \n",
        "            for w in word_tokens: \n",
        "                if w not in stop_words: \n",
        "                    filtered_sentence.append(w) \n",
        "            review=filtered_sentence\n",
        "            train_reviews.append(review)\n",
        "            \n",
        "            label = [0, 0]\n",
        "            label[index] = 1\n",
        "            train_labels.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FrxDiGNV88r"
      },
      "source": [
        "Create array for reveiws and labels for train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLcsQ7IpfdTp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "273d2f21-4813-4660-eef0-c96459701cd4"
      },
      "source": [
        "train_reveiws=np.array(train_reviews)\n",
        "print(train_reveiws.shape)\n",
        "train_labels=np.array(train_labels)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoUKb2JtgTp6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26c9fe1e-fa10-4cd8-93ad-08ca81b4bc01"
      },
      "source": [
        "train_reveiws[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airport',\n",
              " 'starts',\n",
              " 'brand',\n",
              " 'new',\n",
              " 'luxury',\n",
              " 'plane',\n",
              " 'loaded',\n",
              " 'valuable',\n",
              " 'paintings',\n",
              " 'belonging',\n",
              " 'rich',\n",
              " 'businessman',\n",
              " 'philip',\n",
              " 'stevens',\n",
              " 'james',\n",
              " 'stewart',\n",
              " 'flying',\n",
              " 'bunch',\n",
              " 'vip',\n",
              " 'estate',\n",
              " 'preparation',\n",
              " 'opened',\n",
              " 'public',\n",
              " 'museum',\n",
              " 'also',\n",
              " 'board',\n",
              " 'stevens',\n",
              " 'daughter',\n",
              " 'julie',\n",
              " 'kathleen',\n",
              " 'quinlan',\n",
              " 'son',\n",
              " 'luxury',\n",
              " 'jetliner',\n",
              " 'takes',\n",
              " 'planned',\n",
              " 'mid',\n",
              " 'air',\n",
              " 'plane',\n",
              " 'hi',\n",
              " 'jacked',\n",
              " 'co',\n",
              " 'pilot',\n",
              " 'chambers',\n",
              " 'robert',\n",
              " 'foxworth',\n",
              " 'two',\n",
              " 'accomplice',\n",
              " 'banker',\n",
              " 'monte',\n",
              " 'markham',\n",
              " 'wilson',\n",
              " 'michael',\n",
              " 'pataki',\n",
              " 'knock',\n",
              " 'passengers',\n",
              " 'crew',\n",
              " 'sleeping',\n",
              " 'gas',\n",
              " 'plan',\n",
              " 'steal',\n",
              " 'valuable',\n",
              " 'cargo',\n",
              " 'land',\n",
              " 'disused',\n",
              " 'plane',\n",
              " 'strip',\n",
              " 'isolated',\n",
              " 'island',\n",
              " 'making',\n",
              " 'descent',\n",
              " 'chambers',\n",
              " 'almost',\n",
              " 'hits',\n",
              " 'oil',\n",
              " 'rig',\n",
              " 'ocean',\n",
              " 'loses',\n",
              " 'control',\n",
              " 'plane',\n",
              " 'sending',\n",
              " 'crashing',\n",
              " 'sea',\n",
              " 'sinks',\n",
              " 'bottom',\n",
              " 'right',\n",
              " 'bang',\n",
              " 'middle',\n",
              " 'bermuda',\n",
              " 'triangle',\n",
              " 'air',\n",
              " 'short',\n",
              " 'supply',\n",
              " 'water',\n",
              " 'leaking',\n",
              " 'flown',\n",
              " 'miles',\n",
              " 'course',\n",
              " 'problems',\n",
              " 'mount',\n",
              " 'survivor',\n",
              " 'await',\n",
              " 'help',\n",
              " 'time',\n",
              " 'fast',\n",
              " 'running',\n",
              " 'also',\n",
              " 'known',\n",
              " 'slightly',\n",
              " 'different',\n",
              " 'tile',\n",
              " 'airport',\n",
              " 'second',\n",
              " 'sequel',\n",
              " 'smash',\n",
              " 'hit',\n",
              " 'disaster',\n",
              " 'thriller',\n",
              " 'airport',\n",
              " 'directed',\n",
              " 'jerry',\n",
              " 'jameson',\n",
              " 'like',\n",
              " 'predecessors',\n",
              " 'say',\n",
              " 'airport',\n",
              " 'sort',\n",
              " 'forgotten',\n",
              " 'classic',\n",
              " 'entertaining',\n",
              " 'although',\n",
              " 'necessarily',\n",
              " 'right',\n",
              " 'reasons',\n",
              " 'three',\n",
              " 'airport',\n",
              " 'films',\n",
              " 'seen',\n",
              " 'far',\n",
              " 'actually',\n",
              " 'liked',\n",
              " 'one',\n",
              " 'best',\n",
              " 'favourite',\n",
              " 'plot',\n",
              " 'three',\n",
              " 'nice',\n",
              " 'mid',\n",
              " 'air',\n",
              " 'hi',\n",
              " 'jacking',\n",
              " 'crashing',\n",
              " 'see',\n",
              " 'oil',\n",
              " 'rig',\n",
              " 'sinking',\n",
              " 'maybe',\n",
              " 'makers',\n",
              " 'trying',\n",
              " 'cross',\n",
              " 'original',\n",
              " 'airport',\n",
              " 'another',\n",
              " 'popular',\n",
              " 'disaster',\n",
              " 'flick',\n",
              " 'period',\n",
              " 'poseidon',\n",
              " 'adventure',\n",
              " 'submerged',\n",
              " 'stays',\n",
              " 'end',\n",
              " 'stark',\n",
              " 'dilemma',\n",
              " 'facing',\n",
              " 'trapped',\n",
              " 'inside',\n",
              " 'either',\n",
              " 'suffocate',\n",
              " 'air',\n",
              " 'runs',\n",
              " 'drown',\n",
              " 'floods',\n",
              " 'doors',\n",
              " 'opened',\n",
              " 'decent',\n",
              " 'idea',\n",
              " 'could',\n",
              " 'made',\n",
              " 'great',\n",
              " 'little',\n",
              " 'disaster',\n",
              " 'flick',\n",
              " 'bad',\n",
              " 'unsympathetic',\n",
              " 'character',\n",
              " 'dull',\n",
              " 'dialogue',\n",
              " 'lethargic',\n",
              " 'set',\n",
              " 'pieces',\n",
              " 'real',\n",
              " 'lack',\n",
              " 'danger',\n",
              " 'suspense',\n",
              " 'tension',\n",
              " 'means',\n",
              " 'missed',\n",
              " 'opportunity',\n",
              " 'rather',\n",
              " 'sluggish',\n",
              " 'plot',\n",
              " 'keeps',\n",
              " 'one',\n",
              " 'entertained',\n",
              " 'odd',\n",
              " 'minutes',\n",
              " 'much',\n",
              " 'happens',\n",
              " 'plane',\n",
              " 'sinks',\n",
              " 'much',\n",
              " 'urgency',\n",
              " 'thought',\n",
              " 'even',\n",
              " 'navy',\n",
              " 'become',\n",
              " 'involved',\n",
              " 'things',\n",
              " 'pick',\n",
              " 'much',\n",
              " 'shots',\n",
              " 'huge',\n",
              " 'ships',\n",
              " 'helicopters',\n",
              " 'flying',\n",
              " 'something',\n",
              " 'lacking',\n",
              " 'george',\n",
              " 'kennedy',\n",
              " 'jinxed',\n",
              " 'airline',\n",
              " 'worker',\n",
              " 'joe',\n",
              " 'patroni',\n",
              " 'back',\n",
              " 'gets',\n",
              " 'couple',\n",
              " 'scenes',\n",
              " 'barely',\n",
              " 'even',\n",
              " 'says',\n",
              " 'anything',\n",
              " 'preferring',\n",
              " 'look',\n",
              " 'worried',\n",
              " 'background',\n",
              " 'home',\n",
              " 'video',\n",
              " 'theatrical',\n",
              " 'version',\n",
              " 'airport',\n",
              " 'run',\n",
              " 'minutes',\n",
              " 'us',\n",
              " 'tv',\n",
              " 'versions',\n",
              " 'add',\n",
              " 'extra',\n",
              " 'hour',\n",
              " 'footage',\n",
              " 'including',\n",
              " 'new',\n",
              " 'opening',\n",
              " 'credits',\n",
              " 'sequence',\n",
              " 'many',\n",
              " 'scenes',\n",
              " 'george',\n",
              " 'kennedy',\n",
              " 'patroni',\n",
              " 'flashbacks',\n",
              " 'flesh',\n",
              " 'character',\n",
              " 'longer',\n",
              " 'rescue',\n",
              " 'scenes',\n",
              " 'discovery',\n",
              " 'another',\n",
              " 'couple',\n",
              " 'dead',\n",
              " 'bodies',\n",
              " 'including',\n",
              " 'navigator',\n",
              " 'would',\n",
              " 'like',\n",
              " 'see',\n",
              " 'extra',\n",
              " 'footage',\n",
              " 'sure',\n",
              " 'could',\n",
              " 'sit',\n",
              " 'near',\n",
              " 'three',\n",
              " 'hour',\n",
              " 'cut',\n",
              " 'airport',\n",
              " 'expected',\n",
              " 'film',\n",
              " 'dated',\n",
              " 'badly',\n",
              " 'horrible',\n",
              " 'fashions',\n",
              " 'interior',\n",
              " 'design',\n",
              " 'choices',\n",
              " 'say',\n",
              " 'toy',\n",
              " 'plane',\n",
              " 'model',\n",
              " 'effects',\n",
              " 'great',\n",
              " 'either',\n",
              " 'along',\n",
              " 'two',\n",
              " 'airport',\n",
              " 'sequels',\n",
              " 'takes',\n",
              " 'pride',\n",
              " 'place',\n",
              " 'razzie',\n",
              " 'award',\n",
              " 'hall',\n",
              " 'shame',\n",
              " 'although',\n",
              " 'think',\n",
              " 'lots',\n",
              " 'worse',\n",
              " 'films',\n",
              " 'reckon',\n",
              " 'little',\n",
              " 'harsh',\n",
              " 'action',\n",
              " 'scenes',\n",
              " 'little',\n",
              " 'dull',\n",
              " 'unfortunately',\n",
              " 'pace',\n",
              " 'slow',\n",
              " 'much',\n",
              " 'excitement',\n",
              " 'tension',\n",
              " 'generated',\n",
              " 'shame',\n",
              " 'reckon',\n",
              " 'could',\n",
              " 'pretty',\n",
              " 'good',\n",
              " 'film',\n",
              " 'made',\n",
              " 'properly',\n",
              " 'production',\n",
              " 'values',\n",
              " 'alright',\n",
              " 'nothing',\n",
              " 'spectacular',\n",
              " 'acting',\n",
              " 'great',\n",
              " 'two',\n",
              " 'time',\n",
              " 'oscar',\n",
              " 'winner',\n",
              " 'jack',\n",
              " 'lemmon',\n",
              " 'said',\n",
              " 'since',\n",
              " 'mistake',\n",
              " 'star',\n",
              " 'one',\n",
              " 'time',\n",
              " 'oscar',\n",
              " 'winner',\n",
              " 'james',\n",
              " 'stewart',\n",
              " 'looks',\n",
              " 'old',\n",
              " 'frail',\n",
              " 'also',\n",
              " 'one',\n",
              " 'time',\n",
              " 'oscar',\n",
              " 'winner',\n",
              " 'lee',\n",
              " 'grant',\n",
              " 'looks',\n",
              " 'drunk',\n",
              " 'sir',\n",
              " 'christopher',\n",
              " 'lee',\n",
              " 'given',\n",
              " 'little',\n",
              " 'plenty',\n",
              " 'familiar',\n",
              " 'faces',\n",
              " 'look',\n",
              " 'airport',\n",
              " 'disaster',\n",
              " 'orientated',\n",
              " 'three',\n",
              " 'airport',\n",
              " 'films',\n",
              " 'far',\n",
              " 'liked',\n",
              " 'ideas',\n",
              " 'behind',\n",
              " 'even',\n",
              " 'bit',\n",
              " 'silly',\n",
              " 'production',\n",
              " 'bland',\n",
              " 'direction',\n",
              " 'help',\n",
              " 'though',\n",
              " 'film',\n",
              " 'sunken',\n",
              " 'plane',\n",
              " 'boring',\n",
              " 'lethargic',\n",
              " 'followed',\n",
              " 'concorde',\n",
              " 'airport']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpANXbFiWQpB"
      },
      "source": [
        "Read test data while preprocessing and removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh2HO6JkbJIW"
      },
      "source": [
        "data_folder=\"/test\"\n",
        "imdb_folder = \"/aclImdb\"\n",
        "test_reviews = []\n",
        "test_labels = []\n",
        "ps = PorterStemmer()\n",
        "for index,sentiment in enumerate([\"/neg/\", \"/pos/\"]):\n",
        "    path = \"./data\"+imdb_folder + data_folder + sentiment\n",
        "    for filename in sorted(os.listdir(path)):\n",
        "        with open(path + filename, 'r') as f:\n",
        "            review = f.read()\n",
        "            review = review.lower()\n",
        "            review = review.rstrip()\n",
        "            review = review.replace(\"<br />\", \" \")\n",
        "            review = re.sub(r\"[^a-z ]\", \" \", review) \n",
        "            review = re.sub(r\" +\", \" \", review)\n",
        "            word_tokens = word_tokenize(review) \n",
        "            filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "            filtered_sentence = [] \n",
        "            for w in word_tokens: \n",
        "                if w not in stop_words: \n",
        "                    filtered_sentence.append(w) \n",
        "            review=filtered_sentence\n",
        "            test_reviews.append(review)\n",
        "            \n",
        "            label = [0, 0]\n",
        "            label[index] = 1\n",
        "            test_labels.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txrnkvA_Bfab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "93f4e726-c463-4df3-d451-0d165f799441"
      },
      "source": [
        "print(test_reviews[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mr', 'costner', 'dragged', 'movie', 'far', 'longer', 'necessary', 'aside', 'terrific', 'sea', 'rescue', 'sequences', 'care', 'characters', 'us', 'ghosts', 'closet', 'costner', 'character', 'realized', 'early', 'forgotten', 'much', 'later', 'time', 'care', 'character', 'really', 'care', 'cocky', 'overconfident', 'ashton', 'kutcher', 'problem', 'comes', 'kid', 'thinks', 'better', 'anyone', 'else', 'around', 'shows', 'signs', 'cluttered', 'closet', 'obstacle', 'appears', 'winning', 'costner', 'finally', 'well', 'past', 'half', 'way', 'point', 'stinker', 'costner', 'tells', 'us', 'kutcher', 'ghosts', 'told', 'kutcher', 'driven', 'best', 'prior', 'inkling', 'foreshadowing', 'magic', 'could', 'keep', 'turning', 'hour']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os15GdUNWUzZ"
      },
      "source": [
        "Create array for reveiws and labels for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nijPfNFap24y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c9aa21a9-5f9a-4007-9529-5a116af1073e"
      },
      "source": [
        "test_reveiws=np.array(test_reviews)\n",
        "print(test_reveiws.shape)\n",
        "test_labels=np.array(test_labels)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZQmHNXzWZFT"
      },
      "source": [
        "Get total number of unique words in train data to apply one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvPCmW6_zB2v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62a89fec-8a7e-4431-bdda-bad3be1460c1"
      },
      "source": [
        "all_words = []\n",
        "for review in train_reveiws:\n",
        "    for word in review:\n",
        "        all_words.append(word)\n",
        "unique_words = set(all_words)\n",
        "vocab_length=len(unique_words)+50\n",
        "print(len(unique_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q24wVcIWgAO"
      },
      "source": [
        "Apply one hot encoding to the train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1uebQiRD2rr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "555df0e0-6c53-4eba-8252-8d49dcb75c17"
      },
      "source": [
        "train_enc_review=[]\n",
        "for review in train_reveiws:\n",
        "    enc_review=[]\n",
        "    enc_word = [one_hot(word, vocab_length) for word in review]\n",
        "    for x in enc_word:\n",
        "        for y in x:\n",
        "            enc_review.append(y)\n",
        "    train_enc_review.append(enc_review)\n",
        "enc_review=np.array(enc_review)\n",
        "print(enc_review.shape)\n",
        "train_enc_review=np.array(train_enc_review)\n",
        "print(train_enc_review.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(91,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb4OSHEDQAVz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "8d8df4dc-0bf2-4351-f5a3-e73a9a863708"
      },
      "source": [
        "print(train_enc_review[0])\n",
        "print(train_enc_review[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[43691, 32618, 3757, 28066, 11019, 68867, 27522, 66235, 65228, 35663, 17511, 66764, 51285, 63000, 68362, 28710, 61453, 16518, 48682, 16796, 6824, 28009, 6269, 32356, 17511, 32537, 61546, 382, 60653, 28405, 54582, 35799, 72808, 6760, 28710, 31257, 24806, 12498, 1500, 64716, 1067, 70158, 50359, 27284, 38068, 37899, 59648, 37240, 1353, 23264, 55504, 53538, 45445, 20827, 42857, 53538, 38882, 4818, 2553, 45671, 49159, 40265, 4074]\n",
            "[47857, 68867, 25429, 18896, 60807, 70285, 40436, 14302, 16276, 29263, 6539, 20797, 54886, 44928, 49678, 66840, 59608, 15318, 6072, 27647, 10746, 6758, 55332, 47193, 29609, 8208, 44928, 51702, 27492, 2018, 11872, 62920, 60807, 62395, 17724, 43770, 47479, 17993, 70285, 60598, 37942, 47510, 5364, 21595, 26023, 72194, 29430, 12753, 11628, 35051, 34052, 9294, 31479, 38156, 52537, 29332, 50318, 20360, 17881, 11206, 72788, 14302, 62592, 62292, 45910, 70285, 63340, 12706, 6030, 54582, 55297, 21595, 31920, 63930, 70790, 49363, 61234, 3238, 43492, 70285, 7970, 69800, 69452, 2705, 26059, 9334, 9821, 38298, 69128, 13061, 17993, 9779, 6017, 60415, 14960, 3095, 45021, 56951, 66845, 9999, 18765, 64772, 64675, 61546, 14735, 40845, 29609, 23599, 60101, 14019, 15519, 47857, 65013, 7298, 27752, 59210, 70115, 31928, 47857, 56924, 10545, 64060, 24635, 61190, 45705, 47857, 52491, 58719, 25065, 71621, 47728, 6551, 9334, 34961, 3419, 47857, 29047, 40265, 38419, 50367, 60182, 63672, 29971, 20899, 468, 3419, 67593, 47479, 17993, 60598, 11609, 69800, 18659, 70790, 49363, 993, 72498, 18153, 25253, 12195, 2676, 47857, 36639, 6607, 70115, 55624, 14160, 51980, 69470, 41879, 32356, 26601, 60897, 28579, 45431, 1506, 20297, 18246, 15141, 17993, 41951, 6644, 38493, 69329, 6758, 1461, 42986, 71034, 30415, 45445, 1653, 70115, 55624, 60418, 64916, 3152, 22544, 24806, 27757, 70365, 20222, 42475, 31796, 35548, 8936, 17200, 31615, 956, 61754, 33094, 62003, 468, 18482, 63672, 9641, 25765, 46752, 36703, 66225, 70285, 2705, 36703, 60083, 13195, 72808, 53603, 46211, 7076, 25825, 30457, 36703, 6409, 9276, 73153, 36099, 59608, 32243, 7288, 31582, 68424, 747, 35259, 6951, 30009, 35999, 25228, 30966, 916, 59627, 5669, 72808, 67884, 4623, 35299, 4366, 32308, 36907, 40877, 55814, 3812, 3105, 47857, 53308, 46752, 53746, 38662, 31970, 33267, 37329, 14839, 48849, 45062, 18896, 27522, 35758, 5445, 72819, 59627, 31582, 68424, 35999, 47536, 4800, 3152, 60333, 56711, 59627, 26080, 36639, 916, 52889, 23422, 45062, 34362, 12498, 24635, 18659, 37329, 48849, 61659, 71034, 13630, 34764, 3419, 14839, 482, 47857, 69242, 38101, 69820, 9182, 8608, 62419, 19849, 6703, 69652, 45705, 54416, 70285, 54167, 4727, 45445, 18246, 53320, 29430, 47857, 5129, 17724, 54118, 59393, 53663, 28754, 40603, 6026, 47728, 1353, 15484, 16559, 29047, 9854, 1653, 52890, 53909, 59627, 1653, 22544, 6269, 37535, 64063, 36703, 26479, 17200, 7730, 6026, 9854, 71034, 52268, 23264, 38101, 30415, 7090, 28629, 72454, 30037, 18724, 72474, 71786, 45445, 29430, 61546, 72176, 46601, 44885, 20320, 65134, 50249, 26392, 39699, 63672, 61546, 72176, 46601, 49678, 66840, 13709, 18545, 59908, 29609, 63672, 61546, 72176, 46601, 15164, 52232, 13709, 27492, 34531, 19504, 15164, 55056, 1653, 60934, 66836, 52882, 4366, 47857, 70115, 23881, 3419, 47857, 29047, 38419, 60182, 67512, 44874, 72808, 43226, 69094, 28629, 65433, 12246, 64675, 12780, 38101, 9571, 70285, 34534, 27757, 34097, 45141, 47857]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16106OBXWs7_"
      },
      "source": [
        "Padding the train data to make it of similar length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5goeliwPrx7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ae71fd46-4f18-42ba-e5b1-fa1cc04e7b45"
      },
      "source": [
        "padded_inputs = pad_sequences(train_enc_review, padding=\"post\",maxlen=2000)\n",
        "print(padded_inputs[0].shape)\n",
        "print(padded_inputs[1].shape)\n",
        "print(padded_inputs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000,)\n",
            "(2000,)\n",
            "(25000, 2000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI-unp67WpIV"
      },
      "source": [
        "Define the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KicX1j60RWe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "8ed6e703-ddd1-499b-a390-2e3660c693c3"
      },
      "source": [
        "max_len=2000\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_length,20,input_length=max_len))\n",
        "model.add(layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(250, activation='relu'))\n",
        "model.add(layers.Dense(2, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_19 (Embedding)     (None, 2000, 20)          1463300   \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, 2000, 32)          1952      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling (None, 1000, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 32000)             0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 250)               8000250   \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 2)                 502       \n",
            "=================================================================\n",
            "Total params: 9,466,004\n",
            "Trainable params: 9,466,004\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p-0rE6SVQGw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "181b7adf-440a-44bb-b725-940366f1e54e"
      },
      "source": [
        "print(padded_inputs.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 2000)\n",
            "(25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZSqdb6yW7m9"
      },
      "source": [
        "Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJEaE30aSolr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "3a4616f7-f8e2-4986-b4aa-3b466d2e6aec"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(padded_inputs,train_labels,epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 159s 6ms/step - loss: 0.3742 - accuracy: 0.8203\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 157s 6ms/step - loss: 0.1437 - accuracy: 0.9490\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 157s 6ms/step - loss: 0.0579 - accuracy: 0.9815\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0202 - accuracy: 0.9949\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 155s 6ms/step - loss: 0.0083 - accuracy: 0.9977\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 157s 6ms/step - loss: 0.0120 - accuracy: 0.9959\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 157s 6ms/step - loss: 0.0072 - accuracy: 0.9979\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 157s 6ms/step - loss: 0.0077 - accuracy: 0.9973\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 159s 6ms/step - loss: 0.0054 - accuracy: 0.9983\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 155s 6ms/step - loss: 0.0040 - accuracy: 0.9986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdcXWn_VXIQV"
      },
      "source": [
        "Apply one hot encoding to the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ku8x1Kainiv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b65dd284-6336-428c-b728-0cc2ee9f150d"
      },
      "source": [
        "test_enc_review=[]\n",
        "for review in test_reveiws:\n",
        "    enc_review=[]\n",
        "    enc_word = [one_hot(word, vocab_length) for word in review]\n",
        "    for x in enc_word:\n",
        "        for y in x:\n",
        "            enc_review.append(y)\n",
        "    test_enc_review.append(enc_review)\n",
        "enc_review=np.array(enc_review)\n",
        "print(enc_review.shape)\n",
        "test_enc_review=np.array(test_enc_review)\n",
        "print(test_enc_review.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(119,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqCzzoFej-qR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "995d2131-d554-4f21-8bcd-587f29b1060f"
      },
      "source": [
        "print(test_enc_review[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[59262, 5707, 27543, 8802, 38419, 60333, 36193, 39183, 65228, 69452, 56711, 65671, 41448, 39670, 53746, 21542, 41213, 5707, 3152, 9763, 18859, 58719, 36703, 69468, 61546, 41448, 3152, 42844, 41448, 2879, 6580, 70107, 54670, 51054, 16384, 7651, 69990, 59648, 61224, 4065, 25368, 34366, 24710, 42050, 41213, 46254, 13506, 10365, 5707, 43631, 34492, 44279, 2048, 26583, 67819, 45758, 5707, 3035, 53746, 54670, 21542, 11179, 54670, 31561, 29971, 35278, 11020, 61071, 48441, 71034, 17113, 67374, 14839]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKtEixyeXNbJ"
      },
      "source": [
        "Padding the test data to make it of similar length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVDeAarNjHwo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "92593126-aeee-4e8e-e284-721f49d9b072"
      },
      "source": [
        "padded_test = pad_sequences(test_enc_review, padding=\"post\",maxlen=2000)\n",
        "print(padded_test[0].shape)\n",
        "print(padded_test[1].shape)\n",
        "print(padded_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000,)\n",
            "(2000,)\n",
            "(25000, 2000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLAlgnNWXR4w"
      },
      "source": [
        "Evaluate the model for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kGiX38_khZs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "00053605-8210-4736-e7de-464522d47ff3"
      },
      "source": [
        "scores = model.evaluate(padded_test, test_labels, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 83.26%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}